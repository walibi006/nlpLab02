{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UTNGn_9NwgzW"
   },
   "source": [
    "# Stemming\n",
    "\n",
    "Let's see an example of stemming text using [NLTK](https://www.nltk.org/) (Natural Language ToolKit). We will use their SnowballStemmer implementation. The implementation is available online, so if you are curious about how stemming is done in different languages, you can look [here](https://www.nltk.org/api/nltk.stem.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zYDwL5inwK72"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "id": "BGY0wkxVwOfQ",
    "outputId": "99bcb8b9-1b55-4355-b9f2-14134eedfa12"
   },
   "outputs": [],
   "source": [
    "# We need to download a package for word tokenization\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "w-J8ad0rxN5I",
    "outputId": "6ae07d2a-2c79-4dcb-9508-c03195273c3e"
   },
   "outputs": [],
   "source": [
    "text = \"At first, historical linguistics served as the cornerstone of comparative linguistics primarily as a tool for linguistic reconstruction.[5] Scholars were concerned chiefly with establishing language families and reconstructing prehistoric proto-languages, using the comparative method and internal reconstruction.\"\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q3MSl9i5xV5L"
   },
   "source": [
    "Let's start with the word tokenization. Notice how it cut words and symbols."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "6OuOKkwAxSi2",
    "outputId": "f9e8bdd8-5e9a-45c0-ba37-1bbdce0c7a21"
   },
   "outputs": [],
   "source": [
    "\" \".join(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ESYdyL6Txb76"
   },
   "source": [
    "Rule-based tokenizers have to make implementation choices. For example, not splitting hyphenated words (`proto-languages`) or cutting symbols independently (`[ 5 ]`). Different tokenizers will bring different results. \n",
    "\n",
    "NLTK provides several [word and sentence tokenizers](https://www.nltk.org/api/nltk.tokenize.html).\n",
    "\n",
    "## Stemming\n",
    "\n",
    "Now let's apply the stemming to everything that is composed of characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "KTWladxBxaRf",
    "outputId": "322dcb2d-80a7-4684-f7d2-e4d734f0ef51"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "re_word = re.compile(r\"^\\w+$\")\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "stemmed = [stemmer.stem(word) for word in word_tokenize(text.lower()) if re_word.match(word)]\n",
    "        \n",
    "\" \".join(stemmed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wYKiWWQrxlbR"
   },
   "source": [
    "Note how the words are simply cut and stemmed. Note that \"were\" didn't change as it does not follow standard stemming rules.\n",
    "\n",
    "Another example with \"went\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "MCBsfFB8xjAQ",
    "outputId": "c0bf615d-4a07-4904-cb15-b87a05b3497c"
   },
   "outputs": [],
   "source": [
    "text = \" I went to the cinema\"\n",
    "stemmed = [stemmer.stem(word) for word in word_tokenize(text.lower()) if re_word.match(word)]\n",
    "        \n",
    "\" \".join(stemmed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7mxgZJyQ0Prs"
   },
   "source": [
    "## Going Further\n",
    "\n",
    "NLTK proposes several stemming implementation in several languages. Notably, [this little tutorial](http://www.nltk.org/howto/stem.html) shows how to use the `Snowball stemmer` in several languages. You an also directly look into [NLTK's implementation](https://www.nltk.org/_modules/nltk/stem/snowball.html) of different stemmer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "stemming.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
