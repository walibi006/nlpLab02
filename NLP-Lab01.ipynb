{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "VHmIJ8ARPAEj"
   },
   "outputs": [],
   "source": [
    "import datasets as ds\n",
    "import string\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xNcA5a36fkEc"
   },
   "source": [
    "## The Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NRWV6a9Ccntw",
    "outputId": "d398b0b7-2fb8-4be3-8132-5725420f6f06"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['train', 'test', 'unsupervised']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.get_dataset_split_names(\"imdb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oBqfhI_9L-ow"
   },
   "source": [
    "### 1. How many splits does the dataset has?\n",
    "\n",
    "    The dataset is composed of 3 splits : train, test and unsupervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gzdpgvC8c2qz",
    "outputId": "7444679b-beac-4fa2-ee59-60d72bdaa7ad"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset imdb (/home/ethan/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label'],\n",
       "    num_rows: 25000\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_train = ds.load_dataset(\"imdb\", split=\"train\")\n",
    "ds_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2qDvwWGZc53Y",
    "outputId": "a362d7e0-6cb0-4331-ecb7-c07a2fe38a52"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset imdb (/home/ethan/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label'],\n",
       "    num_rows: 25000\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_test = ds.load_dataset(\"imdb\", split=\"test\")\n",
    "ds_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SyRcqtUac-Tk",
    "outputId": "915c8e51-461a-4050-9060-40e218bfbdbb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset imdb (/home/ethan/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label'],\n",
       "    num_rows: 50000\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_unsupervised = ds.load_dataset(\"imdb\", split=\"unsupervised\")\n",
    "ds_unsupervised"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k69BZ5hJMz5L"
   },
   "source": [
    "### 2. How big are these splits? \n",
    "\n",
    "    * train : 25000 \t\n",
    "\n",
    "    * test : 25000 \t\n",
    "\n",
    "    * unsupervised : 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BntYsC8RPrZb",
    "outputId": "d539f4c3-6798-49ed-9fa7-5ccc0bffb9c4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/ethan/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0/cache-568243c224eb1160.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label'],\n",
       "    num_rows: 12500\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_test.filter(lambda d: d['label'] == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s2Mq1Md5fQVP",
    "outputId": "74a1494b-a769-4039-8ff8-62f3c9ee5e9e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/ethan/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0/cache-d036cc1248b6b6bf.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label'],\n",
       "    num_rows: 12500\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_train.filter(lambda d: d['label'] == 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cpy3kBFDN3Ds"
   },
   "source": [
    "### 3. What is the proportion of each class on the supervised splits?\n",
    "\n",
    "    * train : 12500 negatives and 12500 positives\n",
    "\n",
    "    * test : 12500 negatives adn 12500 positives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q85747Wsfq4b"
   },
   "source": [
    "## Naive Bayes classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1mJJf0rpA4Ds"
   },
   "source": [
    "### 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "YFVfZGNZfVPo"
   },
   "outputs": [],
   "source": [
    "def preprocessing(base_text: str):\n",
    "  \"\"\"\n",
    "  Preprocess the text before classification\n",
    "  Args:\n",
    "    base_text: the string to preprocess\n",
    "  Return:\n",
    "    The preprocessed text\n",
    "  \"\"\"\n",
    "  base_text = base_text.lower()\n",
    "  base_text.replace(\"<br />\",' ')\n",
    "  text = \"\"\n",
    "  ponct = string.punctuation\n",
    "  for char in base_text:\n",
    "    if char in ponct:\n",
    "      text += ' '\n",
    "    else:\n",
    "      text += char\n",
    "  return text\n",
    "\n",
    "vectorized_preprocessing = np.vectorize(preprocessing) # vectorizing the function to be faster using numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jEGXzBGZA8__"
   },
   "source": [
    "### 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "bZLFpkinIM8d"
   },
   "outputs": [],
   "source": [
    "def flatten(l):\n",
    "    return [item for sublist in l for item in sublist]\n",
    "\n",
    "def naive_bayes_classifier(document, preprocessed_doc, classes, preprocessed_classes):\n",
    "  \"\"\"\n",
    "  Execute a naive Bayes Classifier on document\n",
    "  Args:\n",
    "    document: the complete dataset from imdb\n",
    "    classes: list of the element of the dataset separated between positives and negatives\n",
    "  Return:\n",
    "    logprior: probability of each class in the dataset in a list\n",
    "    loglikelihood: probabilities of words to belong to a certain class in a list\n",
    "    v: the vocabulary of the dataset\n",
    "  \"\"\"\n",
    "  ndoc = document.num_rows\n",
    "  words_by_texts = flatten([text.split() for text in preprocessed_doc])\n",
    "  v = list(set(words_by_texts)) # list of all the words\n",
    "  logprior = []\n",
    "  loglikelihood = [[], []]\n",
    "  current_class = 0\n",
    "\n",
    "  for i in range(len(classes)):\n",
    "    c = classes[i]\n",
    "    nc = c.num_rows\n",
    "    logprior.append(math.log(nc / ndoc))\n",
    "    bigdoc = flatten([text.split() for text in preprocessed_classes[i]])\n",
    "\n",
    "    # creating an histogram\n",
    "    histo = {}\n",
    "    for word in bigdoc:\n",
    "      if word in histo:\n",
    "        histo[word] += 1\n",
    "      else:\n",
    "        histo[word] = 1\n",
    "\n",
    "    for voc_word in v:\n",
    "      if voc_word in histo:\n",
    "        count = histo[voc_word]\n",
    "        loglikelihood[current_class].append(math.log((count+1)/(len(bigdoc) + len(v))))\n",
    "      else:\n",
    "        loglikelihood[current_class].append(math.log(1/(len(bigdoc) + len(v))))\n",
    "    current_class+=1\n",
    "\n",
    "  return logprior,loglikelihood,v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HGMUmxfnTYmU",
    "outputId": "b28a5ad7-777d-46e0-b412-8111c719c0a5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/ethan/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0/cache-d5f51b718c020c21.arrow\n",
      "Loading cached processed dataset at /home/ethan/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0/cache-d036cc1248b6b6bf.arrow\n"
     ]
    }
   ],
   "source": [
    "classes_preprocessed = vectorized_preprocessing(np.array([ds_train.filter(lambda d: d['label'] == 0)['text'],  ds_train.filter(lambda d: d['label'] == 1)['text']]))\n",
    "document_preprocessed = vectorized_preprocessing(np.array(ds_train['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J0qNr8RO2IaU",
    "outputId": "80fa3147-47b6-48d6-8936-2607d4362e97"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/ethan/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0/cache-d5f51b718c020c21.arrow\n",
      "Loading cached processed dataset at /home/ethan/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0/cache-d036cc1248b6b6bf.arrow\n"
     ]
    }
   ],
   "source": [
    "classes = [ds_train.filter(lambda d: d['label'] == 0),  ds_train.filter(lambda d: d['label'] == 1)]\n",
    "logprior, loglikelihood, v = naive_bayes_classifier(ds_train, document_preprocessed, classes, classes_preprocessed)\n",
    "loglikelihood_dictionnarry = {}\n",
    "for i in range(len(v)):\n",
    "  loglikelihood_dictionnarry[v[i]] = (loglikelihood[0][i], loglikelihood[1][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "Xhp9kB40_uBu"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def get_value(x):\n",
    "    if x in loglikelihood_dictionnarry:\n",
    "      return (loglikelihood_dictionnarry[x][0], loglikelihood_dictionnarry[x][1])\n",
    "    return (0,0)\n",
    "get_values = np.vectorize(get_value)\n",
    "def testbayes(doc, logprior, loglikelihood_dictionnarry, preprocess = True):\n",
    "  \"\"\"\n",
    "  Predict a class with the naive bayes classifier\n",
    "  Args:\n",
    "    doc: the string to classify\n",
    "    logprior: probability of each class in the dataset in a list\n",
    "    loglikelihood: probabilities of words to belong to a certain class in a list\n",
    "    classes: list of the element of the dataset separated between positives and negatives\n",
    "    v: the vocabulary of the dataset\n",
    "  Return:\n",
    "    The predicted class of the string 'doc'\n",
    "  \"\"\"\n",
    "  sum0, sum1 = (logprior[0], logprior[1])\n",
    "  splitted_doc = np.array(doc.split())\n",
    "  sum_values = get_values(splitted_doc)\n",
    "  sum0, sum1 = sum0 + sum_values[0].sum(), sum1 + sum_values[1].sum()\n",
    "  return 0 if sum0 > sum1 else 1\n",
    "\n",
    "def test_bayes_list(x, logprior, loglikelihood_dictionnarry):\n",
    "  return np.array([testbayes(xi, logprior, loglikelihood_dictionnarry, preprocess = False) for xi in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GeXKAMyaeKK6",
    "outputId": "ff081d05-a1df-4bc9-daaf-e640fe6ff290"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/ethan/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0/cache-b72ebb77c49bacbc.arrow\n",
      "Loading cached processed dataset at /home/ethan/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0/cache-568243c224eb1160.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negatives (should be 0)\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "positive (should be 1)\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "classes = vectorized_preprocessing(np.array([ds_test.filter(lambda d: d['label'] == 0)['text'],  ds_test.filter(lambda d: d['label'] == 1)['text']]))\n",
    "print(\"negatives (should be 0)\")\n",
    "print(testbayes(classes[0][0],logprior,loglikelihood_dictionnarry))\n",
    "print(testbayes(classes[0][1],logprior,loglikelihood_dictionnarry))\n",
    "print(testbayes(classes[0][2],logprior,loglikelihood_dictionnarry))\n",
    "print(testbayes(classes[0][100],logprior,loglikelihood_dictionnarry))\n",
    "print(testbayes(classes[0][50],logprior,loglikelihood_dictionnarry))\n",
    "print(testbayes(classes[0][200],logprior,loglikelihood_dictionnarry))\n",
    "print(testbayes(classes[0][250],logprior,loglikelihood_dictionnarry))\n",
    "print(\"positive (should be 1)\")\n",
    "print(testbayes(classes[1][0],logprior,loglikelihood_dictionnarry))\n",
    "print(testbayes(classes[1][1],logprior,loglikelihood_dictionnarry))\n",
    "print(testbayes(classes[1][2],logprior,loglikelihood_dictionnarry))\n",
    "print(testbayes(classes[1][100],logprior,loglikelihood_dictionnarry))\n",
    "print(testbayes(classes[1][50],logprior,loglikelihood_dictionnarry))\n",
    "print(testbayes(classes[1][200],logprior,loglikelihood_dictionnarry))\n",
    "print(testbayes(classes[1][250],logprior,loglikelihood_dictionnarry))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZAXrQpJoBANx"
   },
   "source": [
    "### 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 126
    },
    "id": "Zq0YTqNc_xj1",
    "outputId": "05d2f1af-e329-45c4-8ed6-42b551105e2d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;countvectorizer&#x27;, CountVectorizer()),\n",
       "                (&#x27;multinomialnb&#x27;, MultinomialNB())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;countvectorizer&#x27;, CountVectorizer()),\n",
       "                (&#x27;multinomialnb&#x27;, MultinomialNB())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('countvectorizer', CountVectorizer()),\n",
       "                ('multinomialnb', MultinomialNB())])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import numpy as np\n",
    "\n",
    "sklearn_bayes = make_pipeline(CountVectorizer(), MultinomialNB())\n",
    "sklearn_bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "nRu5wrMKA1t9"
   },
   "outputs": [],
   "source": [
    "model = sklearn_bayes.fit(X=vectorized_preprocessing(np.array(ds_train['text'])),  y=np.array(ds_train['label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JkYJkZwaFUkY",
    "outputId": "4140dd8f-f63d-4b5b-aab5-13126abaee80"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/ethan/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0/cache-b72ebb77c49bacbc.arrow\n",
      "Loading cached processed dataset at /home/ethan/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0/cache-568243c224eb1160.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negatives (should be 0)\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[1]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "positive (should be 1)\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n",
      "[1]\n"
     ]
    }
   ],
   "source": [
    "classes = vectorized_preprocessing(np.array([ds_test.filter(lambda d: d['label'] == 0)['text'],  ds_test.filter(lambda d: d['label'] == 1)['text']]))\n",
    "print(\"negatives (should be 0)\")\n",
    "print(model.predict([classes[0][0]]))\n",
    "print(model.predict([classes[0][1]]))\n",
    "print(model.predict([classes[0][2]]))\n",
    "print(model.predict([classes[0][100]]))\n",
    "print(model.predict([classes[0][50]]))\n",
    "print(model.predict([classes[0][200]]))\n",
    "print(model.predict([classes[0][250]]))\n",
    "print(\"positive (should be 1)\")\n",
    "print(model.predict([classes[1][0]]))\n",
    "print(model.predict([classes[1][1]]))\n",
    "print(model.predict([classes[1][2]]))\n",
    "print(model.predict([classes[1][100]]))\n",
    "print(model.predict([classes[1][50]]))\n",
    "print(model.predict([classes[1][200]]))\n",
    "print(model.predict([classes[1][250]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tVKwp4QLdDIi"
   },
   "source": [
    "### 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "A60vO-4YJdty"
   },
   "outputs": [],
   "source": [
    "X_train = vectorized_preprocessing(np.array(ds_train['text']))\n",
    "Y_train = np.array(ds_train['label'])\n",
    "\n",
    "X_test = vectorized_preprocessing(np.array(ds_test['text']))\n",
    "Y_test = np.array(ds_test['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "OY84D1QHJ2Xr"
   },
   "outputs": [],
   "source": [
    "y_train_pred_student = test_bayes_list(X_train, logprior, loglikelihood_dictionnarry)\n",
    "y_train_pred_scikit = model.predict(X_train)\n",
    "y_test_pred_student = test_bayes_list(X_test, logprior, loglikelihood_dictionnarry)\n",
    "y_test_pred_scikit = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pFV439RaFbC3",
    "outputId": "ad323c30-6d16-4d60-9cbb-d842a2872f8d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy student: train: 0.89364, test: 0.80924\n",
      "Accuracy scikit: train: 0.89808, test: 0.8136\n"
     ]
    }
   ],
   "source": [
    "import sklearn.metrics as metrics\n",
    "\n",
    "train_accurracy_student = metrics.accuracy_score(Y_train, y_train_pred_student)\n",
    "train_accurracy_scikit = metrics.accuracy_score(Y_train, y_train_pred_scikit)\n",
    "test_accurracy_student = metrics.accuracy_score(Y_test, y_test_pred_student)\n",
    "test_accurracy_scikit = metrics.accuracy_score(Y_test, y_test_pred_scikit)\n",
    "\n",
    "print(f\"Accuracy student: train: {train_accurracy_student}, test: {test_accurracy_student}\")\n",
    "print(f\"Accuracy scikit: train: {train_accurracy_scikit}, test: {test_accurracy_scikit}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VgCmloOzdU6w"
   },
   "source": [
    "### 5. Why does scikit-learn implementation have a better Accuracy ?\n",
    "    \n",
    "    The slight difference in accuracy is probably due to scikit-learn using the accuracy value during training to have a better model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QboykQU9eQs1"
   },
   "source": [
    "### 6. Why is accuracy a sufficient measure of evaluation here ?\n",
    "\n",
    "    With imbalanced data-sets, accuracy is not a good measure, since it does not distinguish between the numbers of correctly classified examples of different classes. In this case, the classes are perfectly balanced, so the accuracy gives a good insight of the model's reliability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cwzaZJs1g5r9"
   },
   "source": [
    "7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "IKpHoveDg6e7"
   },
   "outputs": [],
   "source": [
    "Error = []\n",
    "for i in range(len(X_test)):\n",
    "  pred = model.predict([X_test[i]])[0]\n",
    "  if pred != Y_test[i]:\n",
    "    Error.append((pred, ds_test[i]['text']))\n",
    "  if len(Error) == 2:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mKDs7eTzhMaj",
    "outputId": "ff7cc8a5-e9bc-418e-9821-cf41522fbae7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1,\n",
       " \"Blind Date (Columbia Pictures, 1934), was a decent film, but I have a few issues with this film. First of all, I don't fault the actors in this film at all, but more or less, I have a problem with the script. Also, I understand that this film was made in the 1930's and people were looking to escape reality, but the script made Ann Sothern's character look weak. She kept going back and forth between suitors and I felt as though she should have stayed with Paul Kelly's character in the end. He truly did care about her and her family and would have done anything for her and he did by giving her up in the end to fickle Neil Hamilton who in my opinion was only out for a good time. Paul Kelly's character, although a workaholic was a man of integrity and truly loved Kitty (Ann Sothern) as opposed to Neil Hamilton, while he did like her a lot, I didn't see the depth of love that he had for her character. The production values were great, but the script could have used a little work.\")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Error[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MfAIP04-kGh9",
    "outputId": "fc094f82-8057-42e1-b40d-4a23a910c66b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1,\n",
       " \"Ben, (Rupert Grint), is a deeply unhappy adolescent, the son of his unhappily married parents. His father, (Nicholas Farrell), is a vicar and his mother, (Laura Linney), is ... well, let's just say she's a somewhat hypocritical soldier in Jesus' army. It's only when he takes a summer job as an assistant to a foul-mouthed, eccentric, once-famous and now-forgotten actress Evie Walton, (Julie Walters), that he finally finds himself in true 'Harold and Maude' fashion. Of course, Evie is deeply unhappy herself and it's only when these two sad sacks find each other that they can put their mutual misery aside and hit the road to happiness.<br /><br />Of course it's corny and sentimental and very predictable but it has a hard side to it, too and Walters, who could sleep-walk her way through this sort of thing if she wanted, is excellent. It's when she puts the craziness to one side and finds the pathos in the character, (like hitting the bottle and throwing up in the sink), that she's at her best. The problem is she's the only interesting character in the film (and it's not because of the script which doesn't do anybody any favours). Grint, on the other hand, isn't just unhappy; he's a bit of a bore as well while Linney's starched bitch is completely one-dimensional. (Still, she's got the English accent off pat). The best that can be said for it is that it's mildly enjoyable - with the emphasis on the mildly.\")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Error[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fNlAbxtUkltJ"
   },
   "source": [
    "Those two examples were predicted positives althought they were negatives. We should take a look a a true positive to figure out the similitudes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "id": "oPVK1dy2k8s-",
    "outputId": "2e604c4d-b44e-43c5-d370-605f49282e40"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/ethan/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0/cache-568243c224eb1160.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Previous reviewer Claudio Carvalho gave a much better recap of the film's plot details than I could. What I recall mostly is that it was just so beautiful, in every sense - emotionally, visually, editorially - just gorgeous.  If you like movies that are wonderful to look at, and also have emotional content to which that beauty is relevant, I think you will be glad to have seen this extraordinary and unusual work of art.  On a scale of 1 to 10, I'd give it about an 8.75. The only reason I shy away from 9 is that it is a mood piece. If you are in the mood for a really artistic, very romantic film, then it's a 10. I definitely think it's a must-see, but none of us can be in that mood all the time, so, overall, 8.75.\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_test.filter(lambda d: d['label'] == 1)[0]['text'].replace('<br />', ' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f_unoFAbmL4B"
   },
   "source": [
    "For the first one, we can say that it has been wrongly set as positive because the writer is conceding some good aspect to the film  even while he didn't liked it in then end. However, for the second one, The reviewer said things that happen positively during the film which has been considered by the model as a positive review of the film instead of a review of someone who didn't liked it. To sum up, in these two cases, there are many positives wortds which made the model classify them as positives reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J45FN11aneO9"
   },
   "source": [
    "## Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hCU_ZY8Ynif-"
   },
   "source": [
    "### 1. We choose lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2AVr5GorpCUQ",
    "outputId": "ef9b11fd-ebcc-4de5-e3a2-edc082c9b0a5"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# loading the small English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "Ganvidvznjm5"
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "  \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocessing_lemma(base_text: str):\n",
    "  \"\"\"\n",
    "  Preprocess the text before classification\n",
    "  Args:\n",
    "    base_text: the string to preprocess\n",
    "  Return:\n",
    "    The preprocessed text\n",
    "  \"\"\"\n",
    "  base_text = base_text.lower()\n",
    "  base_text.replace(\"<br />\",' ')\n",
    "  text = \"\"\n",
    "  ponct = string.punctuation\n",
    "  for char in base_text:\n",
    "    if char in ponct:\n",
    "      text += ' '\n",
    "    else:\n",
    "      text += char\n",
    "  lemmas = [token.lemma_ for token in nlp(text.lower())]\n",
    "  return \" \".join(lemmas)\n",
    "\n",
    "vectorized_preprocessing_lemma = np.vectorize(preprocessing_lemma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n6kl9F42oS4m"
   },
   "source": [
    "### 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x2nmskA47RiV",
    "outputId": "5f4a934e-27ee-4c23-f280-4836d03f583b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/ethan/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0/cache-d5f51b718c020c21.arrow\n",
      "Loading cached processed dataset at /home/ethan/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0/cache-d036cc1248b6b6bf.arrow\n"
     ]
    }
   ],
   "source": [
    "classes_preprocessed = vectorized_preprocessing_lemma(np.array([ds_train.filter(lambda d: d['label'] == 0)['text'],  ds_train.filter(lambda d: d['label'] == 1)['text']]))\n",
    "document_preprocessed = vectorized_preprocessing_lemma(np.array(ds_train['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JEYNoHoE7anm",
    "outputId": "ac953e2c-417b-4782-e0cc-72124f6427f6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/ethan/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0/cache-d5f51b718c020c21.arrow\n",
      "Loading cached processed dataset at /home/ethan/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0/cache-d036cc1248b6b6bf.arrow\n"
     ]
    }
   ],
   "source": [
    "classes = [ds_train.filter(lambda d: d['label'] == 0),  ds_train.filter(lambda d: d['label'] == 1)]\n",
    "logprior, loglikelihood, v = naive_bayes_classifier(ds_train, document_preprocessed, classes, classes_preprocessed)\n",
    "loglikelihood_dictionnarry = {}\n",
    "for i in range(len(v)):\n",
    "  loglikelihood_dictionnarry[v[i]] = (loglikelihood[0][i], loglikelihood[1][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "4FwhELOQ7mHy"
   },
   "outputs": [],
   "source": [
    "X_train = document_preprocessed.copy()\n",
    "Y_train = np.array(ds_train['label'])\n",
    "\n",
    "X_test = vectorized_preprocessing_lemma(np.array(ds_test['text']))\n",
    "Y_test = np.array(ds_test['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "xF2X3v7Cpagg"
   },
   "outputs": [],
   "source": [
    "y_train_pred_student = test_bayes_list(X_train, logprior, loglikelihood_dictionnarry)\n",
    "y_test_pred_student = test_bayes_list(X_test, logprior, loglikelihood_dictionnarry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8llvHFMfp8ba",
    "outputId": "026463c0-5d52-4467-ef2a-3335030d98a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy student: train: 0.88632, test: 0.80516\n"
     ]
    }
   ],
   "source": [
    "train_accurracy_student = metrics.accuracy_score(Y_train, y_train_pred_student)\n",
    "test_accurracy_student = metrics.accuracy_score(Y_test, y_test_pred_student)\n",
    "\n",
    "print(f\"Accuracy student: train: {train_accurracy_student}, test: {test_accurracy_student}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. We can see that the accuracy obtained with the lemmatization is worse than without it, why ?\n",
    "    \n",
    "    The drop in the accurracy value is probably due to the loss of meanings, especially of tenses, due to lemmatization which does not work in favor of the naive bayes model. Indeed we can see that this loss of meaning (which in some contexts where the word order is respected can be beneficial) makes the model slightly less good."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
